from __future__ import annotations

from pathlib import Path
import re
from typing import List, Dict, Any, Tuple

import fitz  # PyMuPDF
import chromadb
import hanlp
from ollama import embed, chat
from tqdm import tqdm


# 1) HanLP 句子切分器（EOS）
# 预训练模型常量：UD_CTB_EOS_MUL  :contentReference[oaicite:9]{index=9}
split_sent = hanlp.load(hanlp.pretrained.eos.UD_CTB_EOS_MUL)

EMBED_MODEL = "dengcao/Qwen3-Embedding-0.6B:Q8_0"
LLM_MODEL = "Qwen3:0.6B"


def extract_pages(pdf_path: str) -> List[Dict[str, Any]]:
    doc = fitz.open(pdf_path)
    pages = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        text = page.get_text("text")  # 对纯文本 PDF 通常够用
        pages.append({"page": i + 1, "text": text})
    return pages


def clean_text(t: str) -> str:
    t = t.replace("\u00a0", " ")
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()


def chunk_by_sentences(
    sentences: List[str],
    max_chars: int = 900,
    overlap_sents: int = 2
) -> List[str]:
    """
    先分句，再按字符长度拼 chunk（对中文 PoC 很实用）。
    """
    chunks = []
    buf: List[str] = []
    buf_len = 0

    for s in sentences:
        s = s.strip()
        if not s:
            continue

        # 如果当前句子太长，也要单独成块（避免死循环）
        if len(s) > max_chars:
            if buf:
                chunks.append("".join(buf).strip())
                buf, buf_len = [], 0
            chunks.append((s + "\n").strip())
            continue

        if buf and (buf_len + len(s) > max_chars):
            chunks.append("".join(buf).strip())
            # overlap：保留最后几句
            buf = buf[-overlap_sents:] if overlap_sents > 0 else []
            buf_len = sum(len(x) for x in buf)

        buf.append(s + "\n")
        buf_len += len(s)

    if buf:
        chunks.append("".join(buf).strip())

    return chunks


def batch_embed(texts: List[str], batch_size: int = 32) -> List[List[float]]:
    vectors: List[List[float]] = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        # Ollama embed 支持 input 为数组 :contentReference[oaicite:10]{index=10}
        resp = embed(model=EMBED_MODEL, input=batch)
        vectors.extend(resp["embeddings"])
    return vectors


def build_chroma_index(
    pdf_paths: List[str],
    persist_dir: str = "../data/chroma",
    collection_name: str = "gyn_kb",
) -> None:
    client = chromadb.PersistentClient(path=persist_dir)  # 本地持久化 :contentReference[oaicite:11]{index=11}
    collection = client.get_or_create_collection(
        name=collection_name,
        metadata={"hnsw:space": "cosine"},  # cosine/l2/ip :contentReference[oaicite:12]{index=12}
    )

    for pdf_path in pdf_paths:
        pdf_path = str(pdf_path)
        book_name = Path(pdf_path).name
        stem = Path(pdf_path).stem

        pages = extract_pages(pdf_path)

        ids, docs, metas = [], [], []

        for p in tqdm(pages, desc=f"Parsing {book_name}"):
            page_text = clean_text(p["text"])
            if not page_text:
                continue

            # HanLP 分句：predict(data) 做 sentence split :contentReference[oaicite:13]{index=13}
            sentences = split_sent.predict(page_text)
            chunks = chunk_by_sentences(sentences, max_chars=900, overlap_sents=2)

            for ci, chunk in enumerate(chunks):
                doc_id = f"{stem}_p{p['page']}_c{ci}"
                ids.append(doc_id)
                docs.append(chunk)
                metas.append({
                    "source": book_name,
                    "page": p["page"],
                    "chunk": ci,
                })

        if not docs:
            continue

        vectors = batch_embed(docs, batch_size=32)

        # upsert：不存在则创建，存在则更新 :contentReference[oaicite:14]{index=14}
        collection.upsert(
            ids=ids,
            documents=docs,
            metadatas=metas,
            embeddings=vectors,
        )

    print("Index built.")


def answer_question(
    question: str,
    persist_dir: str = "../data/chroma",
    collection_name: str = "gyn_kb",
    top_k: int = 6,
) -> str:
    client = chromadb.PersistentClient(path=persist_dir)
    collection = client.get_collection(name=collection_name)

    q_vec = embed(model=EMBED_MODEL, input=question)["embeddings"][0]

    res = collection.query(
        query_embeddings=[q_vec],
        n_results=top_k,
        include=["documents", "metadatas", "distances"],
    )

    docs = res["documents"][0]
    metas = res["metadatas"][0]

    context_blocks = []
    for i, (d, m) in enumerate(zip(docs, metas), start=1):
        context_blocks.append(f"[{i}] 来源：{m['source']} 第{m['page']}页\n{d}")
    context = "\n\n".join(context_blocks)

    system_prompt = (
        "你是面向女性用户的妇科健康科普助手。\n"
        "你只能基于【资料】回答；如果资料不足，请明确说“资料不足以回答”。\n"
        "不要进行诊断、不要给出具体处方/药物剂量。\n"
        "若用户描述可能的紧急情况或高风险症状，提醒尽快就医或急诊。\n"
        "回答最后输出“参考来源：……”并列出引用的页码。\n"
    )

    user_prompt = (
        f"问题：{question}\n\n"
        f"资料：\n{context}\n\n"
        "请用中文回答，并尽量引用资料中的表述（但不要大段照抄）。"
    )

    resp = chat(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )
    return resp["message"]["content"]


if __name__ == "__main__":
    pdfs = [
        "../data/pdfs/妇产科学.pdf",
    ]

    # 先跑一次建库
    build_chroma_index(pdfs)

    # 再问一句
    print(answer_question("什么是细菌性阴道炎？有哪些典型表现？"))
